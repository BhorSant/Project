{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "mDgbUHAGgjLW",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "pEMng2IbBLp7",
        "yiiVWRdJDDil",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhorSant/Project/blob/main/Appliance_Energy_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Authour :** Bhor Santosh Baburav"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data set is at 10 min for about 4.5 months. The house temperature and humidity conditions were monitored with a ZigBee wireless sensor network. Each wireless node transmitted the temperature and humidity conditions around 3.3 min. Then, the wireless data was averaged for 10 minutes. The energy data was logged every 10 minutes with m-bus energy metres. Weather from the nearest airport weather station (Chievres Airport, Belgium) was downloaded from a public data set from Reliable Prognosis (rp5.ru) and merged together with the experimental data sets using the date and time column. Two random variables have been included in the data set for testing the regression models and to filter out non-predictive attributes. You need to predict the energy use of appliances."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The task is to develop a regression model to predict the energy consumption of appliances in a residential setting based on a comprehensive dataset collected over 4.5 months. The dataset includes monitored house temperature and humidity conditions obtained through a ZigBee wireless sensor network, with each node transmitting data approximately every 3.3 minutes. The wireless data has been averaged over 10-minute intervals. Additionally, energy data, logged every 10 minutes using m-bus energy meters, is part of the dataset.**\n",
        "\n",
        "**The dataset has been enriched by merging it with weather data from the nearest airport weather station (Chievres Airport, Belgium), obtained from a public dataset from Reliable Prognosis (rp5.ru). The merging was done using the date and time columns. To assess the robustness of regression models, two random variables have been included in the dataset.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv(\"/content/data_application_energy.csv\")"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print('The number of rows in dataset is - ' , df.shape[0])\n",
        "print('The number of columns in dataset is - ' , df.shape[1])"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.isnull().sum().sort_index(ascending=True)"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(df.isnull(), cmap='Reds')\n",
        "plt.title('Missing Values in the Dataset')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset have 19735 Rows  and 29 columns and no any null values or missing value in the dataset."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The observation data consists of the following variables:**\n",
        "\n",
        "datetime year-month-day hour : minute:second\n",
        "\n",
        "Appliances: energy use in Wh [TARGETED]\n",
        "\n",
        "lights: energy use of light fixtures in the house in Wh\n",
        "\n",
        "T1: Temperature in kitchen area, in Celsius\n",
        "\n",
        "RH_1: Humidity in kitchen area, in %\n",
        "\n",
        "T2: Temperature in living room area, in Celsius\n",
        "\n",
        "RH_2:Humidity in living room area, in %\n",
        "\n",
        "T3:Temperature in laundry room area\n",
        "\n",
        "RH_3:Humidity in laundry room area, in %\n",
        "\n",
        "T4:Temperature in office room, in Celsius\n",
        "\n",
        "RH_4:Humidity in office room, in %\n",
        "\n",
        "T5:Temperature in bathroom, in Celsius\n",
        "\n",
        "RH_5:Humidity in bathroom, in %\n",
        "\n",
        "T6:Temperature outside the building (north side), in Celsius\n",
        "\n",
        "RH_6:Humidity outside the building (north side), in %\n",
        "\n",
        "T7:Temperature in ironing room , in Celsius\n",
        "\n",
        "RH_7:Humidity in ironing room, in %\n",
        "\n",
        "T8:Temperature in teenager room 2, in Celsius\n",
        "\n",
        "RH_8:Humidity in teenager room 2, in %\n",
        "\n",
        "T9:Temperature in parents room, in Celsius\n",
        "\n",
        "RH_9:Humidity in parents room, in %\n",
        "\n",
        "T_out:Temperature outside (from Chièvres weather station), in Celsius\n",
        "\n",
        "Press_mm_hg: (from Chièvres weather station), in mm Hg\n",
        "\n",
        "RH_out: Humidity outside (from Chièvres weather station), in %\n",
        "\n",
        "Windspeed: (from Chièvres weather station), in m/s\n",
        "\n",
        "Visibility: (from Chièvres weather station), in km\n",
        "\n",
        "Tdewpoint: (from Chièvres weather station), °C\n",
        "\n",
        "rv1: Random variable 1, nondimensional\n",
        "\n",
        "rv2: Rnadom variable 2, nondimensional"
      ],
      "metadata": {
        "id": "MQbbyyiAnPNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset has the 28 columns has the numerical values in the datset for identify the mean , min, max, standrtdevaition, 25%,50%, 75% are the all columns are statistical describe."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for column in df.columns:\n",
        "    unique_values = df[column].nunique()\n",
        "    print(f\"Unique values for {column}:\\n{unique_values}\\n\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    print(f\"Unique values for {column}:\\n{unique_values}\\n\")"
      ],
      "metadata": {
        "id": "YxfC-OoYowDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given this is not timeseries problem and we will focus on predicting the apliance consumputions, we can ignore column"
      ],
      "metadata": {
        "id": "CVAlMvNJ3Mma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "zmn9J9KF334I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "X = df.drop([\"Appliances\"], axis=1)\n",
        "y = df['Appliances']\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.lights.value_counts()"
      ],
      "metadata": {
        "id": "pDOsQHDI8C1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Appliances.value_counts()"
      ],
      "metadata": {
        "id": "PyYnSEs-8zJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to map current column names to new column names\n",
        "column_mapping = {'T1': 'KITCHEN_TEMP',\n",
        "    'RH_1': 'KITCHEN_HUM',\n",
        "    'T2': 'LIVING_TEMP',\n",
        "    'RH_2' :'LIVING_HUM',\n",
        "    'T3': 'BEDROOM_TEMP',\n",
        "    'RH_3':'BEDROOM_HUM',\n",
        "    'T4' : 'OFFICE_TEMP',\n",
        "    'RH_4' : 'OFFICE_HUM',\n",
        "    'T5' : 'BATHROOM_TEMP',\n",
        "    'RH_5': 'BATHROOM_HUM',\n",
        "    'T6':'OUTSIDE_TEMP_build',\n",
        "    'RH_6': 'OUTSIDE_HUM_build',\n",
        "    'T7': 'IRONING_ROOM_TEMP',\n",
        "    'RH_7' : 'IRONING_ROOM_HUM',\n",
        "    'T8' :'TEEN_ROOM_2_TEMP',\n",
        "    'RH_8' : 'TEEN_ROOM_HUM',\n",
        "    'T9': 'PARENTS_ROOM_TEMP',\n",
        "    'RH_9': 'PARENTS_ROOM_HUM',\n",
        "    'T_out' :'OUTSIDE_TEMP_wstn',\n",
        "    'RH_out' :'OUTSIDE_HUM_wstn'}\n",
        "\n",
        "# Rename the columns using the mapping\n",
        "df.rename(columns=column_mapping, inplace=True)"
      ],
      "metadata": {
        "id": "h388QzyXs0fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_columns\",None)"
      ],
      "metadata": {
        "id": "j9K24jBitLyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "0cIe0K7ss5Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame with a column 'your_datetime_column' containing datetime values\n",
        "# If not, you can convert a column to datetime using: df['your_datetime_column'] = pd.to_datetime(df['your_datetime_column'])\n",
        "\n",
        "# Convert the column to datetime format\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Set the datetime column as the index\n",
        "df.set_index('date', inplace=True)\n",
        "\n",
        "# Create new columns for day, hour, month, week, and year\n",
        "df['day'] = df.index.day\n",
        "df['hour'] = df.index.hour\n",
        "df['month'] = df.index.month\n",
        "df['week'] = df.index.isocalendar().week\n",
        "df['year'] = df.index.year\n",
        "\n",
        "# Optionally, you can also create a 'day_of_week' column (0 = Monday, 1 = Tuesday, ..., 6 = Sunday)\n",
        "df['day_of_week'] = df.index.dayofweek\n",
        "\n",
        "# Display the modified DataFrame\n",
        "print(df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "gdM7vWkmwAn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index('date', inplace=True)"
      ],
      "metadata": {
        "id": "VCJqpFKyDJZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "5i_CnN2D-cUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temperature columns - The house temperature is between the 14.89 to 29.89 deg. I identify the temperature change in all T1-T8\n",
        "\n",
        "Humidiy columns - Humidity inside house varies is between 20.60% to 63.36% with exception of RH_5 (Bathroom) and RH_6 (Outside house) which varies between 29.82% to 96.32% and 1% to 99.9% respectively.\n",
        "\n",
        "Appliances - 75% of Appliance consumption is less than 100 Wh . With the maximum consumption of 1080 Wh , there will be outliers in this column and there are small number of cases where consumption is very high\n",
        "\n",
        "Lights column - Intially I believed lights column will be able to give useful information . With 11438 0 (zero) enteries in 14801 rows , this column will not add any value to the model . I believed light consumption along with humidity level in a room will give idea about human presence in the room and hence its impact on Appliance consumption. Hence for now , I will dropping this column\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "2AplR9JOy-la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "import plotly.express as px\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "# To understand the timeseries variation of the applaince energy consumption\n",
        "visData = go.Scatter( x= df.date  ,  mode = \"lines\", y = df.Appliances )\n",
        "layout = go.Layout(title = 'Appliance energy consumption measurement' , xaxis=dict(title='Date'), yaxis=dict(title='Appliancces (Wh)'))\n",
        "fig = go.Figure(data=[visData],layout=layout)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This year, we aim to identify the monthly and daily power consumption trends over a four-month period, providing a detailed breakdown of how much power is used each month and on a day-to-day basis."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart found the some data for the data in the monthly identified power and date of the each date and how much power consumption are to show in this chart.\n",
        "\n",
        "Each value on the chart serves as crucial data, facilitating its integration into transformers or connection meters. This automated process helps save and identify data efficiently, optimizing the utilization of the chart for effective data management."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.Appliances.mean()"
      ],
      "metadata": {
        "id": "fBMccjIQAIe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart visually represents each data point, allowing for a clear understanding of values. This visualization can be leveraged in transformers or connection meters to automatically capture and save data, aiding in the identification and analysis of information.\n",
        "\n",
        "Each value on the chart serves as crucial data, facilitating its integration into transformers or connection meters. This automated process helps save and identify data efficiently, optimizing the utilization of the chart for effective data management."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "fig = X.hist(bins=20,figsize=(12,16))\n",
        "print(fig)"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It produces a set of histograms for visualizing the distributions of data. Histograms are used to display the frequency or count of data points within specified bins or intervals. They help you understand the data's central tendency, spread, and underlying patterns. It's a common way to visualize the distribution of a single variable and identify features like modes, skewness, or presence of multiple subpopulations in the data. This type of plot is used to explore and understand the shape and characteristics of individual variables in your dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart are very useful in the modelling  assumption, feature Engineering, Outliers, data Undersadning are this topic cover in the following:\n",
        "\n",
        "Modeling Assumptions: When using statistical models that assume normality (e.g., linear regression), features not following normal distribution may violate model assumptions. This can affect the model's accuracy and reliability.\n",
        "\n",
        "Feature Engineering: Non-normally distributed features may require transformations (e.g., log, square root) to make them more closely resemble a normal distribution. This can improve model performance.\n",
        "\n",
        "Outliers: Non-normally distributed features may indicate the presence of outliers or extreme values that need to be addressed, as they can influence model outcomes.\n",
        "\n",
        "Data Understanding: It's important to understand the nature of the underlying distributions for each feature to make informed decisions about data preprocessing and model selection.\n",
        "\n",
        "Skewed Data: Skewed distributions (e.g., right-skewed or left-skewed) can affect the interpretation of statistical metrics and may require different statistical techniques.\n",
        "\n"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many pairs in a pair plot exhibit heteroscedasticity (unequal variance) between variables, it suggests that the spread or variability of one variable changes with the values of another variable. This can indicate that the relationship between those variables is not constant across the entire range of values.\n",
        "\n",
        "Insights you can gain from observing heteroscedasticity in pair plots:\n",
        "\n",
        "Potential Issues: Heteroscedasticity can be problematic for regression models, as they often assume homoscedasticity (constant variance). Detecting heteroscedasticity alerts you to the need for potential model adjustments.\n",
        "\n",
        "Modeling Choices: You might need to consider alternative models that can handle heteroscedastic data better, like robust regression techniques.\n",
        "\n",
        "Feature Engineering: It may be necessary to transform or engineer the variables to mitigate heteroscedasticity, such as applying logarithmic transformations or normalizing the data.\n",
        "\n",
        "Outliers: Heteroscedasticity can sometimes be attributed to outliers. Identifying these outliers and understanding their impact on the relationship between variables is essential.\n",
        "\n",
        "Subgroup Analysis: You might need to analyze subsets of your data where heteroscedasticity is less pronounced to better understand the relationships within those subsets."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Create a pivot table to aggregate the daily energy counsption\n",
        "mask = np.zeros_like(df.corr())\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "with sns.axes_style(\"white\"):\n",
        "  f, ax = plt.subplots(figsize=(15, 15))\n",
        "  ax = sns.heatmap(df.corr(), mask=mask, vmax=1, vmin=-1, center=0, annot=True)"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart used in the code is a heatmap. Heatmaps are chosen for this type of data visualization for several reasons:\n",
        "\n",
        "Data Aggregation: The data is being aggregated into a pivot table that summarizes daily energy consumption across different months and days. Heatmaps are effective for visualizing aggregated data and patterns.\n",
        "\n",
        "Visualizing Two Dimensions: Heatmaps are well-suited for displaying two dimensions of data simultaneously, in this case, 'month' and 'day.' Each cell in the heatmap represents a combination of these two dimensions.\n",
        "\n",
        "Color Encoding: Heatmaps use color to encode the values in each cell, making it easy to differentiate between high and low values. In this code, warmer colors (yellow-green) represent higher energy consumption, while cooler colors (blue) represent lower consumption.\n",
        "\n",
        "Insight into Patterns: Heatmaps are excellent for identifying patterns and trends in data, such as seasonal variations or correlations between days and months.\n",
        "\n",
        "Customization: The code provides customization options, such as setting the title, labels, and the color map (cmap), which allows for tailoring the visualization to the specific dataset and objectives."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mixed-color heatmap showing varying energy consumption across 5 months and 2 to 5 high-energy consumption days in each month provides several insights:\n",
        "\n",
        "Seasonal Patterns: The heatmap suggests that there are specific periods or seasons within each month when energy consumption is notably higher. This could be attributed to weather conditions, special occasions, or other external factors.\n",
        "\n",
        "Weekends vs. Weekdays: The heatmap might reveal a recurring pattern of higher energy consumption on weekends (e.g., Saturdays and Sundays) compared to weekdays. This could be due to increased activities or appliance usage on weekends.\n",
        "\n",
        "Outliers and Anomalies: Days with exceptionally high energy consumption (e.g., spikes) are clearly visible as isolated cells with warm colors. Investigating these outliers can help identify reasons behind sudden surges in energy usage.\n",
        "\n",
        "Energy Efficiency: The mixed heatmap suggests that there are days with moderate to lower energy consumption, indicating potential opportunities for improving energy efficiency. Analyzing these lower-consumption days can help in understanding what practices or conditions lead to reduced energy usage.\n",
        "\n",
        "Month-to-Month Variation: The heatmap allows for a visual comparison of energy consumption across different months. If a particular month consistently stands out with high consumption days, it may be linked to seasonal variations or specific factors unique to that month.\n",
        "\n",
        "Decision Support: Understanding these patterns and insights from the heatmap can be valuable for decision-making, such as optimizing energy management strategies, scheduling maintenance, or implementing energy-saving measures."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Business Impacts:\n",
        "\n",
        "* Energy Optimization: Understanding when and why energy consumption is high allows businesses to optimize their energy usage during peak periods, potentially reducing costs and environmental impact.\n",
        "\n",
        "* Cost Savings: Identifying opportunities for reducing energy consumption on lower-usage days can lead to cost savings in the long run. Implementing energy-efficient practices or technology can be financially beneficial.\n",
        "\n",
        "* Resource Allocation: Recognizing seasonal patterns helps with allocating resources effectively. For instance, if specific months have consistently high energy consumption, businesses can plan for extra staffing, maintenance, or supplies.\n",
        "\n",
        "* Predictive Maintenance: By pinpointing outliers in energy usage, organizations can proactively address issues related to appliances or systems that may be driving high consumption, leading to longer equipment lifespans and reduced downtime.\n",
        "\n",
        "Negative Business Impacts:\n",
        "\n",
        "* Operational Challenges: The insights may reveal that energy consumption is consistently high during months where the business operations require it. In such cases, reducing energy usage may negatively impact productivity.\n",
        "\n",
        "* Seasonal Variability: If the high-energy consumption months are due to unavoidable external factors (e.g., extreme weather conditions), it may be challenging to mitigate the effects, leading to increased operational costs and potential service disruptions.\n",
        "\n",
        "* Capital Investment: Implementing energy-efficient technologies or practices may require significant upfront investments. The negative impact could be felt in the short term before cost savings are realized."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "daily_energy = df.pivot_table(values='Appliances', index='day', columns='month', aggfunc = 'mean')\n",
        "\n",
        "# Create a heatmap using the pivot table\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Daily Energy Consumption')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Day')\n",
        "plt.imshow(daily_energy, cmap='YlGnBu', aspect='auto')\n",
        "plt.colorbar(label='Energy Consumption')\n",
        "plt.xticks(range(0,5), ['Jan', 'Feb', 'Mar', 'Apr', 'May'])\n",
        "plt.yticks(range(1, 32))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p2inqFUS8CSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart used in the code is a heatmap. Heatmaps are chosen for this type of data visualization for several reasons:\n",
        "\n",
        "Data Aggregation: The data is being aggregated into a pivot table that summarizes daily energy consumption across different months and days. Heatmaps are effective for visualizing aggregated data and patterns.\n",
        "\n",
        "Visualizing Two Dimensions: Heatmaps are well-suited for displaying two dimensions of data simultaneously, in this case, 'month' and 'day.' Each cell in the heatmap represents a combination of these two dimensions.\n",
        "\n",
        "Color Encoding: Heatmaps use color to encode the values in each cell, making it easy to differentiate between high and low values. In this code, warmer colors (yellow-green) represent higher energy consumption, while cooler colors (blue) represent lower consumption.\n",
        "\n",
        "Insight into Patterns: Heatmaps are excellent for identifying patterns and trends in data, such as seasonal variations or correlations between days and months.\n",
        "\n",
        "Customization: The code provides customization options, such as setting the title, labels, and the color map (cmap), which allows for tailoring the visualization to the specific dataset and objectives."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mixed-color heatmap showing varying energy consumption across 5 months and 2 to 5 high-energy consumption days in each month provides several insights:\n",
        "\n",
        "Seasonal Patterns: The heatmap suggests that there are specific periods or seasons within each month when energy consumption is notably higher. This could be attributed to weather conditions, special occasions, or other external factors.\n",
        "\n",
        "Weekends vs. Weekdays: The heatmap might reveal a recurring pattern of higher energy consumption on weekends (e.g., Saturdays and Sundays) compared to weekdays. This could be due to increased activities or appliance usage on weekends.\n",
        "\n",
        "Outliers and Anomalies: Days with exceptionally high energy consumption (e.g., spikes) are clearly visible as isolated cells with warm colors. Investigating these outliers can help identify reasons behind sudden surges in energy usage.\n",
        "\n",
        "Energy Efficiency: The mixed heatmap suggests that there are days with moderate to lower energy consumption, indicating potential opportunities for improving energy efficiency. Analyzing these lower-consumption days can help in understanding what practices or conditions lead to reduced energy usage.\n",
        "\n",
        "Month-to-Month Variation: The heatmap allows for a visual comparison of energy consumption across different months. If a particular month consistently stands out with high consumption days, it may be linked to seasonal variations or specific factors unique to that month.\n",
        "\n",
        "Decision Support: Understanding these patterns and insights from the heatmap can be valuable for decision-making, such as optimizing energy management strategies, scheduling maintenance, or implementing energy-saving measures."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the heatmap can potentially have both positive and negative business impacts, depending on how they are leveraged and addressed. Here's an evaluation of potential outcomes:\n",
        "\n",
        "Positive Business Impacts:\n",
        "\n",
        "Energy Optimization: Understanding when and why energy consumption is high allows businesses to optimize their energy usage during peak periods, potentially reducing costs and environmental impact.\n",
        "\n",
        "Cost Savings: Identifying opportunities for reducing energy consumption on lower-usage days can lead to cost savings in the long run. Implementing energy-efficient practices or technology can be financially beneficial.\n",
        "\n",
        "Resource Allocation: Recognizing seasonal patterns helps with allocating resources effectively. For instance, if specific months have consistently high energy consumption, businesses can plan for extra staffing, maintenance, or supplies.\n",
        "\n",
        "Predictive Maintenance: By pinpointing outliers in energy usage, organizations can proactively address issues related to appliances or systems that may be driving high consumption, leading to longer equipment lifespans and reduced downtime.\n",
        "\n",
        "Negative Business Impacts:\n",
        "\n",
        "Operational Challenges: The insights may reveal that energy consumption is consistently high during months where the business operations require it. In such cases, reducing energy usage may negatively impact productivity.\n",
        "\n",
        "Seasonal Variability: If the high-energy consumption months are due to unavoidable external factors (e.g., extreme weather conditions), it may be challenging to mitigate the effects, leading to increased operational costs and potential service disruptions.\n",
        "\n",
        "Capital Investment: Implementing energy-efficient technologies or practices may require significant upfront investments. The negative impact could be felt in the short term before cost savings are realized.\n",
        "\n",
        "User Comfort: Efforts to reduce energy consumption may result in discomfort for occupants (e.g., temperature adjustments). Striking the right balance between energy efficiency and user comfort is essential."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = df.describe().columns"
      ],
      "metadata": {
        "id": "wD1ShBLcFwKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "# plot a bar plot for each categorical feature count (except Date)\n",
        "\n",
        "for col in numeric_features[1:]:\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    feature = df[col]\n",
        "    feature.hist(bins=50, ax = ax)\n",
        "    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "    ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spread of data points across the Histogram and the relatively horizontal regression line suggest that there is a weak or nearly no linear relationship between indoor or outdoor temperatures and energy consumption.\n",
        "\n",
        "Insights:\n",
        "\n",
        "Weak Relationship: The horizontal regression line indicates that as indoor or outdoor temperatures change, there is little effect on energy consumption. This implies that energy consumption is not significantly driven by temperature variations, at least within the range observed in the dataset.\n",
        "\n",
        "Non-Linearity: The wide dispersion of data points suggests that other factors likely contribute to energy consumption variations. Non-linear relationships or interactions with other variables might be at play.\n",
        "\n",
        "No Clear Trend: There's no apparent trend indicating that energy consumption consistently increases or decreases with temperature changes. This lack of a consistent pattern is important for managing energy efficiently.\n",
        "\n",
        "Other Influencing Factors: The weak relationship emphasizes the importance of considering other factors like occupancy, time of day, or appliance usage patterns that might be more significant in explaining energy consumption variations.\n"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the heatmap can potentially have both positive and negative business impacts, depending on how they are leveraged and addressed. Here's an evaluation of potential outcomes:\n",
        "\n",
        "Positive Business Impacts:\n",
        "\n",
        "Energy Optimization: Understanding when and why energy consumption is high allows businesses to optimize their energy usage during peak periods, potentially reducing costs and environmental impact.\n",
        "\n",
        "Cost Savings: Identifying opportunities for reducing energy consumption on lower-usage days can lead to cost savings in the long run. Implementing energy-efficient practices or technology can be financially beneficial.\n",
        "\n",
        "Resource Allocation: Recognizing seasonal patterns helps with allocating resources effectively. For instance, if specific months have consistently high energy consumption, businesses can plan for extra staffing, maintenance, or supplies.\n",
        "\n",
        "Predictive Maintenance: By pinpointing outliers in energy usage, organizations can proactively address issues related to appliances or systems that may be driving high consumption, leading to longer equipment lifespans and reduced downtime.\n",
        "\n",
        "Negative Business Impacts:\n",
        "\n",
        "Operational Challenges: The insights may reveal that energy consumption is consistently high during months where the business operations require it. In such cases, reducing energy usage may negatively impact productivity.\n",
        "\n",
        "Seasonal Variability: If the high-energy consumption months are due to unavoidable external factors (e.g., extreme weather conditions), it may be challenging to mitigate the effects, leading to increased operational costs and potential service disruptions.\n",
        "\n",
        "Capital Investment: Implementing energy-efficient technologies or practices may require significant upfront investments. The negative impact could be felt in the short term before cost savings are realized.\n",
        "\n",
        "User Comfort: Efforts to reduce energy consumption may result in discomfort for occupants (e.g., temperature adjustments). Striking the right balance between energy efficiency and user comfort is essential."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "def label_point(x, y, val, ax):\n",
        "    a = pd.concat({'x': x, 'y': y, 'val': val}, axis=1)\n",
        "    for i, point in a.iterrows():\n",
        "        ax.text(point['x'], point['y'], str(point['val']))"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'numeric_features' is a list of numeric features in your DataFrame\n",
        "# Also assuming 'df' is your DataFrame\n",
        "\n",
        "for col in numeric_features[1:-1]:\n",
        "    fig, ax = plt.subplots(figsize=(9, 6))\n",
        "\n",
        "    feature = df[col]\n",
        "    label = df['Appliances']\n",
        "\n",
        "    correlation = feature.corr(label)\n",
        "\n",
        "    plt.scatter(x=feature, y=label, alpha=0.5)  # Added alpha for transparency\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Appliances')\n",
        "    ax.set_title('Appliances vs ' + col + ' - correlation: ' + str(correlation))\n",
        "\n",
        "    # Fit a linear regression line\n",
        "    z = np.polyfit(df[col], df['Appliances'], 1)\n",
        "    y_hat = np.poly1d(z)(df[col])\n",
        "    plt.plot(df[col], y_hat, \"r--\", lw=1)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "RUFAVTYAGPC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose scatter plots with regression lines to visualize the relationship between indoor and outdoor temperatures (in different rooms) and energy consumption. These charts are valuable because they allow us to:\n",
        "\n",
        "Identify Relationships: Scatter plots help us visually assess whether there's any apparent correlation between indoor/outdoor temperatures and energy consumption.\n",
        "\n",
        "Regression Line: The regression line provides insights into the direction and strength of the relationship. For example, if the line has a positive slope, it indicates that as temperatures increase, energy consumption tends to increase.\n",
        "\n",
        "Multiple Features: I created multiple charts to explore the relationship with various indoor and outdoor temperatures, providing a comprehensive view of the impact of temperature on energy usage in different parts of a building.\n",
        "\n",
        "Alpha for Transparency: I used 'alpha' to make the individual data points semi-transparent, making it easier to see dense areas of data."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spread of data points across the scatter plot and the relatively horizontal regression line suggest that there is a weak or nearly no linear relationship between indoor or outdoor temperatures and energy consumption.\n",
        "\n",
        "Insights:\n",
        "\n",
        "Weak Relationship: The horizontal regression line indicates that as indoor or outdoor temperatures change, there is little effect on energy consumption. This implies that energy consumption is not significantly driven by temperature variations, at least within the range observed in the dataset.\n",
        "\n",
        "Non-Linearity: The wide dispersion of data points suggests that other factors likely contribute to energy consumption variations. Non-linear relationships or interactions with other variables might be at play.\n",
        "\n",
        "No Clear Trend: There's no apparent trend indicating that energy consumption consistently increases or decreases with temperature changes. This lack of a consistent pattern is important for managing energy efficiently."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights suggest no strong correlation between temperature and energy consumption. This can be positive if businesses aim to optimize energy use through other means, like appliance efficiency and usage patterns. However, a negative impact could occur if a business invests in temperature control systems expecting significant energy savings based on temperature alone. Careful consideration of factors is essential."
      ],
      "metadata": {
        "id": "OgMfg9fxDrol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "OzTzw9XtJgs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the column to datetime format\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Set the datetime column as the index\n",
        "df.set_index('date', inplace=True)"
      ],
      "metadata": {
        "id": "mcFIdNjLD9OT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['weekday'] = df.index.weekday"
      ],
      "metadata": {
        "id": "znvrHf0hKFj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.reset_index('date', inplace=True)"
      ],
      "metadata": {
        "id": "gG7Pm2zkETR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Assuming 'data' is your DataFrame with relevant columns (e.g., 'weekday' and 'Appliances')\n",
        "# You can create a line chart to compare energy consumption on weekdays vs. weekends\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Group the data by 'weekday' and calculate the mean energy consumption for weekdays and weekends\n",
        "weekday_energy = df[df['weekday'] < 5].groupby('hour')['Appliances'].mean()\n",
        "weekend_energy = df[df['weekday'] >= 5].groupby('hour')['Appliances'].mean()\n",
        "\n",
        "# Plot energy consumption for weekdays and weekends\n",
        "plt.plot(weekday_energy.index, weekday_energy.values, label='Weekdays', marker='o')\n",
        "plt.plot(weekend_energy.index, weekend_energy.values, label='Weekends', marker='o')\n",
        "\n",
        "plt.title('Energy Consumption on Weekdays vs. Weekends')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Mean Energy Consumption')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this chart because it effectively compares energy consumption patterns between weekdays and weekends, providing insights into how energy usage varies based on the day of the week. It helps identify whether there are differences in energy consumption behavior during workdays (weekdays) and non-workdays (weekends). This information can be valuable for businesses and households to make informed decisions regarding energy management and resource allocation"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight from the chart is that energy consumption patterns significantly differ between weekdays and weekends. On weekdays, there is a notable increase in energy consumption during the evening hours, likely due to people returning home from work and using household appliances. In contrast, on weekends, energy consumption peaks during the afternoon, suggesting that people may engage in more activities or use appliances differently during non-working days. This information can help businesses and individuals optimize energy usage and potentially reduce costs during specific times of the week."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can potentially help create a positive business impact. By understanding the differences in energy consumption patterns between weekdays and weekends, businesses and utility providers can develop strategies to optimize energy production and distribution. For example, they can adjust energy generation and distribution schedules to meet the varying demands throughout the week. This optimization can lead to more efficient energy utilization, reduced costs, and potentially a more environmentally friendly operation.\n",
        "\n",
        "However, it's essential to consider potential negative impacts. For instance, if businesses or utility providers do not adapt to these insights and continue to supply energy uniformly throughout the week, it could lead to overproduction, increased costs, and potentially negative environmental consequences. Therefore, the key is to leverage these insights to make informed decisions that align energy supply with demand patterns for overall positive business and environmental impacts."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# To group the data by hour and calculate the mean energy consumption for each hour\n",
        "hourly_energy = df.groupby('hour')['Appliances'].mean()\n",
        "\n",
        "# Create a line chart to visualize the hourly energy consumption patterns\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(hourly_energy.index, hourly_energy.values, marker='o', linestyle='-')\n",
        "plt.title('Hourly Energy Consumption Patterns')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Energy Consumption (mean)')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this specific line chart to visualize the hourly energy consumption patterns because it effectively displays how energy consumption varies throughout the day. It helps identify peak hours and trends in energy usage, which is valuable for businesses to optimize operations and potentially reduce energy costs. The chart provides a clear and concise representation of hourly consumption, making it easy to interpret and act upon."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from the chart are as follows:\n",
        "\n",
        "Peak Hours: There is a notable rise in energy consumption from 6 am to 6 pm. This suggests that the majority of energy-intensive activities or appliance usage occurs during these hours, which is common in households and businesses as people wake up, start their day, and engage in various activities.\n",
        "\n",
        "Evening Decline: After 6 pm, there is a decline in energy consumption. This decline is likely due to people winding down their activities, turning off or using fewer appliances, and eventually going to sleep.\n",
        "\n",
        "Midday Dip: There is a small dip in energy consumption between 11 am and 3 pm. This could be attributed to reduced activity during the midday hours when people might be at work or engaged in activities outside the home, resulting in lower energy usage."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can help create a positive business impact in various ways:\n",
        "\n",
        "Positive Business Impact:\n",
        "\n",
        "Cost Savings: Understanding the hourly energy consumption patterns allows businesses to implement strategies to reduce energy usage during peak hours. This can lead to cost savings, especially in commercial and industrial settings where energy costs are significant.\n",
        "\n",
        "Resource Allocation: Businesses can better allocate resources by scheduling energy-intensive tasks or processes during periods of lower energy consumption. This optimization can improve efficiency and reduce operational costs.\n",
        "\n",
        "Sustainability: By identifying periods of high energy consumption, businesses can focus on reducing their carbon footprint during peak hours. This aligns with sustainability goals and environmental responsibility, which is a positive aspect for businesses in today's eco-conscious world."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "fig = px.bar(df, x='day_of_week', y='Appliances')\n",
        "fig.show()\n",
        "fig = px.bar(df, x='hour', y='Appliances')\n",
        "fig.show()\n",
        "fig = px.bar(df, x='day', y='Appliances')\n",
        "fig.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " have chosen to analyze the daily, hourly, and weekly power consumption patterns of various appliances."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. \"Are there noticeable spikes or dips in power consumption at specific times of the day, and if so, what appliances are predominantly contributing to these variations?\"\n",
        "\n",
        "2. \"How does the power consumption vary during weekdays compared to weekends, and what appliances show the most significant differences in usage patterns between these time periods?\"\n",
        "\n",
        "3. \"Are there consistent trends in power usage during peak hours, and how can this information inform energy conservation strategies or demand management?\"\n",
        "\n",
        "4. \"Which appliances exhibit the highest overall power consumption, and are there specific periods when these appliances are most active during the day, hour, or week?\"\n",
        "\n",
        "5. \"Are there correlations between external factors, such as weather conditions or seasonal changes, and fluctuations in power consumption for specific appliances?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can help create a positive business impact in various ways:\n",
        "\n",
        "Positive Business Impact:\n",
        "\n",
        "Cost Savings: Understanding the hourly energy consumption patterns allows businesses to implement strategies to reduce energy usage during peak hours. This can lead to cost savings, especially in commercial and industrial settings where energy costs are significant.\n",
        "\n",
        "Resource Allocation: Businesses can better allocate resources by scheduling energy-intensive tasks or processes during periods of lower energy consumption. This optimization can improve efficiency and reduce operational costs.\n",
        "\n",
        "Sustainability: By identifying periods of high energy consumption, businesses can focus on reducing their carbon footprint during peak hours. This aligns with sustainability goals and environmental responsibility, which is a positive aspect for businesses in today's eco-conscious world."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n"
      ],
      "metadata": {
        "id": "tggltwNcMfqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Create a bubble chart\n",
        "fig = px.scatter(df, x=\"day\", y=\"Appliances\", size=\"Appliances\", color=\"day_of_week\", hover_data=[\"hour\", \"Appliances\"])\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"I opted for bubble plots as they offer a comprehensive visualization strategy, allowing me to assess diverse data types on a day-wise basis. By representing various features through individual bubbles in the same plot, I can easily manipulate and analyze the data, gaining clear insights into the day-wise relationships between different variables such as indoor and outdoor temperatures and energy consumption.\""
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spread of data points across the scatter plot and the relatively horizontal regression line suggest that there is a weak or nearly no linear relationship between indoor or outdoor temperatures and energy consumption.\n",
        "\n",
        "Insights:\n",
        "\n",
        "Weak Relationship: The horizontal regression line indicates that as indoor or outdoor temperatures change, there is little effect on energy consumption. This implies that energy consumption is not significantly driven by temperature variations, at least within the range observed in the dataset.\n",
        "\n",
        "Non-Linearity: The wide dispersion of data points suggests that other factors likely contribute to energy consumption variations. Non-linear relationships or interactions with other variables might be at play.\n",
        "\n",
        "No Clear Trend: There's no apparent trend indicating that energy consumption consistently increases or decreases with temperature changes. This lack of a consistent pattern is important for managing energy efficiently.\n",
        "\n",
        "Other Influencing Factors: The weak relationship emphasizes the importance of considering other factors like occupancy, time of day, or appliance usage patterns that might be more significant in explaining energy consumption variations.\n",
        "\n",
        "Overall, the charts suggest that while temperature certainly plays a role in energy consumption"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose bubble plots with regression lines to visualize the relationship between indoor and outdoor temperatures (in different rooms) and energy consumption. These charts are valuable because they allow us to:\n",
        "\n",
        "Identify Relationships: Scatter plots help us visually assess whether there's any apparent correlation between indoor/outdoor temperatures and energy consumption.\n",
        "\n",
        "Regression Line: The regression line provides insights into the direction and strength of the relationship. For example, if the line has a positive slope, it indicates that as temperatures increase, energy consumption tends to increase.\n",
        "\n",
        "Multiple Features: I created multiple charts to explore the relationship with various indoor and outdoor temperatures, providing a comprehensive view of the impact of temperature on energy usage in different parts of a building.\n",
        "\n",
        "Alpha for Transparency: I used 'alpha' to make the individual data points semi-transparent, making it easier to see dense areas of data."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "#  Assuming 'data' is your DataFrame with the relevant columns (e.g., 'hour' and 'Appliances')\n",
        "# # You can create a line chart to show energy consumption throughout the day\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Group the data by hour and calculate the mean energy consumption for each hour\n",
        "hourly_energy = df.groupby('hour')['Appliances'].mean()\n",
        "\n",
        "# Split the data into daytime (6:00 AM to 6:00 PM) and nighttime (6:00 PM to 6:00 AM)\n",
        "daytime_energy = hourly_energy[6:18]\n",
        "\n",
        "nighttime_energy= hourly_energy[0:6].append(hourly_energy[18:24])\n",
        "\n",
        "# Plot the daytime and nighttime energy consumption\n",
        "plt.plot(daytime_energy.index, daytime_energy.values, label='Daytime', marker='o',color = 'r')\n",
        "plt.plot(nighttime_energy.index, nighttime_energy.values, label='Nighttime', marker='o',color = 'b')\n",
        "\n",
        "plt.title('Energy Consumption Throughout the Day')\n",
        "plt.xlabel('Hour of the Day')\n",
        "plt.ylabel('Mean Energy Consumption')\n",
        "plt.xticks(range(24))\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a line chart to visualize the energy consumption throughout the day, specifically distinguishing between daytime and nighttime consumption. This chart effectively shows how energy usage varies over a 24-hour period, highlighting the differences between daytime and nighttime patterns."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insight from the chart is that there is a noticeable pattern in energy consumption throughout the day. Energy consumption increases during the daytime, with the highest consumption occurring in the afternoon. In contrast, energy consumption decreases during the nighttime hours, reaching its lowest point in the early morning. This suggests that energy usage is influenced by the time of day, with higher demand during daytime hours."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insights can potentially help create a positive business impact. Understanding the pattern of energy consumption throughout the day allows businesses to optimize energy management. For example, they can implement strategies to reduce energy consumption during peak hours, thus reducing energy costs and promoting sustainability. Additionally, businesses can adjust energy production or distribution strategies to meet the varying demand throughout the day, potentially reducing operational costs.\n",
        "\n",
        "However, if businesses do not adapt to these insights and continue to consume energy at a consistent rate, it may lead to negative consequences, such as increased energy costs during peak hours and potential strain on energy infrastructure. Therefore, taking action based on these insights is crucial for a positive business impact."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Assuming 'data' is your DataFrame\n",
        "# Replace column names with your actual column names\n",
        "x = df['KITCHEN_TEMP']\n",
        "y = df['KITCHEN_HUM']\n",
        "z = df['Appliances']\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "ax.scatter(x, y, z, c='blue', marker='o')\n",
        "\n",
        "ax.set_xlabel('Kitchen Temperature')\n",
        "ax.set_ylabel('Kitchen Humidity')\n",
        "ax.set_zlabel('Appliances')\n",
        "\n",
        "ax.set_title('3D Scatter Plot of Kitchen Temperature, Kitchen Humidity, and Appliances')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I opted for a 3D scatter plot to explore and identify patterns in the data of kitchen humidity and temperature. This visualization allows me to examine the intricate interplay between these two variables in a three-dimensional space, offering a more holistic understanding of their relationships and potential dependencies."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without having the specific chart or data details, I can't provide insights about what was observed from the 3D scatter plot of kitchen humidity and temperature. However, here are some general points you might consider looking for or mentioning when analyzing a 3D scatter plot:\n",
        "\n",
        "Patterns in Clusters: Check for any clusters or patterns in the distribution of points. Do certain ranges of humidity and temperature tend to occur together frequently?\n",
        "\n",
        "Correlation: Assess whether there's a noticeable correlation between kitchen humidity and temperature. For example, does an increase in temperature correspond to a certain trend in humidity?\n",
        "\n",
        "Outliers: Identify any outliers that stand out from the main cluster. Outliers may provide valuable information about exceptional conditions or errors in the data.\n",
        "\n",
        "Trends over Time: If your data has a time component, examine whether there are any temporal trends or variations in the relationship between humidity and temperature.\n",
        "\n",
        "Seasonal Effects: Consider whether there are seasonal patterns or variations that might influence kitchen conditions.\n",
        "\n",
        "Data Range Exploration: Look at the range of values covered by the variables. Are they within expected or reasonable bounds?"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without specific details about the insights gained from the 3D scatter plot of kitchen humidity and temperature, it's challenging to determine the potential positive or negative business impacts. However, I can provide some general considerations:\n",
        "\n",
        "**Positive Business Impacts:**\n",
        "1. **Optimization of Environment:** If insights suggest a correlation between specific ranges of humidity and temperature that are associated with optimal conditions, businesses may use this information to optimize the indoor environment for better comfort or productivity.\n",
        "\n",
        "2. **Energy Efficiency:** Understanding patterns in humidity and temperature can contribute to more efficient HVAC system management, potentially leading to energy savings and cost reduction.\n",
        "\n",
        "3. **Predictive Maintenance:** If outliers or patterns in the data suggest abnormal conditions, this information can be used for predictive maintenance, preventing potential issues with appliances or systems.\n",
        "\n",
        "**Potential Negative Impacts:**\n",
        "1. **Increased Operational Costs:** If the insights reveal inefficiencies in temperature or humidity control, it could lead to increased energy consumption and operational costs.\n",
        "\n",
        "2. **Comfort Issues:** In a business setting, uncomfortable conditions due to inappropriate temperature or humidity levels may negatively impact employee well-being and productivity.\n",
        "\n",
        "3. **Equipment Failures:** If abnormal conditions identified in the data are not addressed, it may lead to equipment failures or malfunctions, resulting in downtime and repair costs.\n",
        "\n"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of all columns ending with \"_TEMP\"\n",
        "temperature_columns = [col for col in df.columns if col.endswith('_TEMP')]\n",
        "\n",
        "# Plot histograms for each temperature column\n",
        "for column in temperature_columns:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(df[column], bins=20, color='skyblue', edgecolor='black')\n",
        "\n",
        "    plt.title(f'Histogram of {column}')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(axis='y', alpha=0.75)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I opted for histograms to conduct a thorough analysis of various columns. This choice enables a detailed exploration of the distribution patterns, allowing me to gain insights into the frequency and spread of values across different features."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without specific details about the data or the particular insights you've derived from the histogram analysis of different columns, I can provide general examples of insights that one might gain:\n",
        "\n",
        "1. **Distribution Shapes:** Insights might include observations about the shapes of the distributions. Are they symmetric, skewed, bimodal, or exhibit other characteristics?\n",
        "\n",
        "2. **Central Tendency:** You may gather insights into the central tendency of each column. For instance, where is the peak or center of the distribution?\n",
        "\n",
        "3. **Spread of Values:** Histograms can reveal the spread or variability of values within each column. Are the values tightly clustered, or is there a wide range?\n",
        "\n",
        "4. **Outliers:** The presence of outliers, if any, could be highlighted. Outliers can provide insights into exceptional or unexpected data points.\n",
        "\n",
        "5. **Data Skewness:** Histograms help in identifying whether the data is positively or negatively skewed, providing insights into the direction of the skewness.\n",
        "\n",
        "6. **Multi-Modality:** If a histogram exhibits multiple peaks, it might suggest the presence of different modes or clusters within the data.\n",
        "\n",
        "7. **Patterns over Time:** If the histograms are plotted over time, any trends or patterns in the distribution changes over different periods may be apparent."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insight about non-linear relationships in the correlation heatmap is important for improving model accuracy and predictive power. While it may necessitate more complex modeling techniques, this can lead to more accurate predictions and valuable insights. Therefore, the insights are likely to have a positive business impact by enabling better decision-making and resource optimization, rather than leading to negative growth."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        " ## Correlation\n",
        "plt.figure(figsize=(15,8))\n",
        "correlation = df.corr()\n",
        "sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap is used for visualization to quickly identify relationships between variables. It provides a color-coded matrix where colors represent the strength and direction of correlations. It's a powerful tool for understanding data patterns, identifying key features, and guiding feature selection in machine learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the color of a correlation heatmap indicates a non-linear relationship, it suggests that simple linear models may not capture the underlying data patterns effectively. This insight implies the need for more complex, non-linear modeling techniques or feature engineering to better represent and predict the data.\n",
        "\n",
        "The gained insight about non-linear relationships in the correlation heatmap is important for improving model accuracy and predictive power. While it may necessitate more complex modeling techniques, this can lead to more accurate predictions and valuable insights. Therefore, the insights are likely to have a positive business impact by enabling better decision-making and resource optimization, rather than leading to negative growth."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of column names in your dataset\n",
        "columns = df.columns\n",
        "\n",
        "# Determine the number of rows and columns for subplots\n",
        "num_rows = len(columns)\n",
        "num_cols = 1\n",
        "\n",
        "# Create subplots with specified number of rows and columns\n",
        "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 80))\n",
        "\n",
        "# Iterate over each column (excluding \"Appliances\") and create pair plot\n",
        "for i, column in enumerate(columns):\n",
        "    #if column != \"Appliances\":\n",
        "        sns.scatterplot(data=df, x=\"Appliances\", y=column, ax=axes[i])\n",
        "        axes[i].set_xlabel(\"Appliances\")\n",
        "        axes[i].set_ylabel(column)\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w3JCrpQpvYCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is a visual tool that displays pairwise relationships between variables in a dataset. It's beneficial for exploratory data analysis, revealing patterns, correlations, and outliers. Pair plots help understand how variables interact, guiding feature selection and data preprocessing in machine learning. They are especially useful when dealing with multivariate data, identifying potential associations, and making informed decisions for model building and feature engineering."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many pairs in a pair plot exhibit heteroscedasticity (unequal variance) between variables, it suggests that the spread or variability of one variable changes with the values of another variable. This can indicate that the relationship between those variables is not constant across the entire range of values."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly, I can help you set up three hypothetical statements for hypothesis testing related to appliances energy prediction. Keep in mind that I won't have the actual dataset, so the examples will be hypothetical. Once you have your dataset, you can use statistical testing methods to evaluate these statements.\n",
        "\n",
        "### Hypothetical Statements:\n",
        "\n",
        "1. **Statement 1: The energy consumption of appliances is positively correlated with outdoor temperature.**\n",
        "   - **Null Hypothesis (H0):** There is no correlation between appliance energy consumption and outdoor temperature (correlation coefficient = 0).\n",
        "   - **Alternative Hypothesis (H1):** There is a positive correlation between appliance energy consumption and outdoor temperature (correlation coefficient > 0).\n",
        "\n",
        "2. **Statement 2: The energy consumption of appliances is higher during weekdays compared to weekends.**\n",
        "   - **Null Hypothesis (H0):** There is no difference in energy consumption between weekdays and weekends.\n",
        "   - **Alternative Hypothesis (H1):** Energy consumption of appliances is higher during weekdays compared to weekends.\n",
        "\n",
        "3. **Statement 3: The average energy consumption during morning hours is not significantly different from the average energy consumption during evening hours.**\n",
        "   - **Null Hypothesis (H0):** The average energy consumption during morning hours is equal to the average energy consumption during evening hours.\n",
        "   - **Alternative Hypothesis (H1):** The average energy consumption during morning hours is significantly different from the average energy consumption during evening hours.\n",
        "\n",
        "These hypothetical statements are based on common patterns that might be observed in energy consumption datasets.\n"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1  The energy consumption of appliances is positively correlated with outdoor temperature."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant linear relationship between the independent variables and the appliance energy consumption.\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant linear relationship between the independent variables and the appliance energy consumption."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "r8Rm1l6iG9GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame 'df' with the provided columns\n",
        "\n",
        "# Extract relevant columns\n",
        "energy_consumption = df['Appliances']\n",
        "outdoor_temperature = df['OUTSIDE_TEMP_build']\n",
        "\n",
        "# Perform Pearson correlation test\n",
        "correlation_coefficient, p_value = scipy.stats.pearsonr(energy_consumption, outdoor_temperature)\n",
        "\n",
        "# Output results\n",
        "print(f'Correlation Coefficient: {correlation_coefficient}')\n",
        "print(f'P-Value: {p_value}')\n",
        "\n",
        "# Check significance level (e.g., 0.05)\n",
        "if p_value < 0.05:\n",
        "    print('Reject the null hypothesis. There is a significant correlation between Appliances and OUTSIDE_TEMP_build.')\n",
        "else:\n",
        "    print('Fail to reject the null hypothesis. There is no significant correlation between Appliances and OUTSIDE_TEMP_build.')\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform Pearson correlation test to obtain P-Value."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calculates the Pearson correlation coefficient and p-value for the correlation between energy consumption and outdoor temperature. The null hypothesis is that there is no correlation (correlation coefficient = 0), and the alternative hypothesis is that there is a correlation (correlation coefficient > 0). The p-value is then compared to a chosen significance level to determine the significance of the correlation."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame 'df' with the provided columns\n",
        "\n",
        "# Extract relevant columns\n",
        "energy_consumption_weekdays = df.loc[df['day_of_week'] < 5, 'Appliances']  # Assuming Monday is 0 and Sunday is 6\n",
        "energy_consumption_weekends = df.loc[df['day_of_week'] >= 5, 'Appliances']\n",
        "\n",
        "# Perform two-sample t-test\n",
        "t_statistic, p_value = scipy.stats.ttest_ind(energy_consumption_weekdays, energy_consumption_weekends, equal_var=False)\n",
        "\n",
        "# Output results\n",
        "print(f'T-Statistic: {t_statistic}')\n",
        "print(f'P-Value: {p_value}')\n",
        "\n",
        "# Check significance level (e.g., 0.05)\n",
        "if p_value < 0.05:\n",
        "    print('Reject the null hypothesis. Energy consumption is higher during weekdays compared to weekends.')\n",
        "else:\n",
        "    print('Fail to reject the null hypothesis. There is no significant difference in energy consumption between weekdays and weekends.')\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform two-sample t-test"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose the two-sample t-test for this scenario because you want to compare the mean energy consumption between two independent groups: weekdays and weekends. The t-test is a suitable choice when you are comparing means of two groups and assuming normality of the data.\n",
        "\n",
        "Here are the key reasons for choosing the two-sample t-test:\n",
        "\n",
        "1. **Type of Data:** The t-test is appropriate when dealing with continuous data (like energy consumption) and when the assumptions of normality hold.\n",
        "\n",
        "2. **Comparison of Means:** The hypothesis is concerned with the difference in means between two independent groups (weekdays and weekends).\n",
        "\n",
        "3. **Independence:** The weekdays and weekends are treated as independent groups, and the t-test assumes independence between observations.\n",
        "\n",
        "4. **Assumption of Normality:** While the t-test is fairly robust to violations of normality, it performs well when the data is approximately normally distributed, especially for larger sample sizes.\n",
        "\n",
        "5. **Homogeneity of Variances:** The `ttest_ind` function in `scipy.stats` allows for unequal variances between the two groups, making it suitable for cases where the assumption of equal variances may not hold.\n"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import scipy.stats\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have a DataFrame 'df' with the provided columns\n",
        "\n",
        "# Extract relevant columns\n",
        "energy_consumption_morning = df.loc[(df['hour'] >= 6) & (df['hour'] < 12), 'Appliances']\n",
        "energy_consumption_evening = df.loc[(df['hour'] >= 18) & (df['hour'] < 24), 'Appliances']\n",
        "\n",
        "# Print lengths for debugging\n",
        "print(f'Length of energy_consumption_morning: {len(energy_consumption_morning)}')\n",
        "print(f'Length of energy_consumption_evening: {len(energy_consumption_evening)}')\n",
        "\n",
        "if len(energy_consumption_morning) != len(energy_consumption_evening) or len(energy_consumption_morning) == 0:\n",
        "    raise ValueError('Unequal lengths of arrays or one of the arrays is empty. Check your data.')\n",
        "\n",
        "# Perform paired-sample t-test\n",
        "t_statistic, p_value = scipy.stats.ttest_rel(energy_consumption_morning, energy_consumption_evening)\n",
        "\n",
        "# Output results\n",
        "print(f'T-Statistic: {t_statistic}')\n",
        "print(f'P-Value: {p_value}')\n",
        "\n",
        "# Check significance level (e.g., 0.05)\n",
        "if p_value < 0.05:\n",
        "    print('Reject the null hypothesis. The average energy consumption during morning hours is significantly different from evening hours.')\n",
        "else:\n",
        "    print('Fail to reject the null hypothesis. There is no significant difference in average energy consumption between morning and evening hours.')\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform paired-sample t-test"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This error are developed in this hypothesis because the unequal length of the array Length of energy_consumption_morning: 4932\n",
        "Length of energy_consumption_evening: 4933 that's why error are developed in the program."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "TP8Nj2TqMZjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum().plot(kind='bar')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Number of Missing Values')\n",
        "plt.title('Missing Values in the Dataset')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Kyr1HNgcTSGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No, any missing values in the dataset.**"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "jWUIQuTWWvrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have a DataFrame 'df' with the provided columns\n",
        "# If not, you should load or create your DataFrame here\n",
        "\n",
        "# Define columns where you want to handle outliers\n",
        "columns_to_handle_outliers = ['Appliances', 'lights', 'KITCHEN_TEMP', 'KITCHEN_HUM',\n",
        "                               'LIVING_TEMP', 'LIVING_HUM', 'BEDROOM_TEMP', 'BEDROOM_HUM',\n",
        "                               'OFFICE_TEMP', 'OFFICE_HUM', 'BATHROOM_TEMP', 'BATHROOM_HUM',\n",
        "                               'OUTSIDE_TEMP_build', 'OUTSIDE_HUM_build', 'IRONING_ROOM_TEMP',\n",
        "                               'IRONING_ROOM_HUM', 'TEEN_ROOM_2_TEMP', 'TEEN_ROOM_HUM',\n",
        "                               'PARENTS_ROOM_TEMP', 'PARENTS_ROOM_HUM', 'OUTSIDE_TEMP_wstn',\n",
        "                               'Press_mm_hg', 'OUTSIDE_HUM_wstn', 'Windspeed', 'Visibility',\n",
        "                               'Tdewpoint', 'rv1', 'rv2']\n",
        "\n",
        "# Function to handle outliers using Z-score\n",
        "def handle_outliers_zscore(data, column):\n",
        "    z_scores = (data[column] - data[column].mean()) / data[column].std()\n",
        "    data[column] = np.where(np.abs(z_scores) > 3, data[column].median(), data[column])\n",
        "\n",
        "# Apply outlier treatment to specified columns\n",
        "for column in columns_to_handle_outliers:\n",
        "    handle_outliers_zscore(df, column)\n",
        "\n",
        "# Verify the changes\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize outliers using box plots\n",
        "# Create box plots for specified columns\n",
        "plt.figure(figsize=(16, 10))\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "for i, column in enumerate(columns_to_handle_outliers, start=1):\n",
        "    plt.subplot(5, 6, i)\n",
        "    sns.boxplot(x=df[column], color='skyblue')\n",
        "    plt.title(f'Box Plot - {column}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tuEA8FZKUwq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create interactive box plots using Plotly Express\n",
        "fig = px.box(df, y=columns_to_handle_outliers, title=\"Box Plots for Outlier Visualization\")\n",
        "\n",
        "# Show the interactive plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "J_Zyy7hQVDwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Assuming you have a DataFrame 'df' with the provided columns\n",
        "# If not, make sure to load or create your DataFrame before proceeding\n",
        "\n",
        "# Specify three columns for the 3D scatter plot\n",
        "x_column = 'OUTSIDE_TEMP_build'\n",
        "y_column = 'Windspeed'\n",
        "z_column = 'Appliances'\n",
        "\n",
        "# Create 3D scatter plot using Plotly Express\n",
        "fig = px.scatter_3d(df, x=x_column, y=y_column, z=z_column, title=\"3D Scatter Plot\")\n",
        "\n",
        "# Show the interactive plot\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "Z0u-RR6YYiBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "for ftr in col_list:\n",
        "  print(ftr,'\\n')\n",
        "  q_25= np.percentile(df[ftr], 25)\n",
        "  q_75 = np.percentile(df[ftr], 75)\n",
        "  iqr = q_75 - q_25\n",
        "  print('Percentiles: 25th=%.3f, 75th=%.3f, IQR=%.3f' % (q_25, q_75, iqr))\n",
        "  # calculate the outlier cutoff\n",
        "  cut_off = iqr * 1.5\n",
        "  lower = q_25 - cut_off\n",
        "  upper = q_75 + cut_off\n",
        "  print(f\"\\nlower = {lower} and upper = {upper} \\n \")\n",
        "  # identify outliers\n",
        "  outliers = [x for x in df[ftr] if x < lower or x > upper]\n",
        "  print('Identified outliers: %d' % len(outliers))\n",
        "  #removing outliers\n",
        "  if len(outliers)!=0:\n",
        "\n",
        "    def bin(row):\n",
        "      if row[ftr]> upper:\n",
        "        return upper\n",
        "      if row[ftr] < lower:\n",
        "        return lower\n",
        "      else:\n",
        "        return row[ftr]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J3Kgc4wUZgtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I used the Z-score method for outlier treatment. The Z-score identifies data points that deviate significantly from the mean. I chose this method because it is a widely used and intuitive technique that helps standardize data and identify extreme values based on standard deviations. The threshold of 3 was used to consider data points beyond three standard deviations as outliers. Additionally, the median was used to replace identified outliers, as it is less sensitive to extreme values compared to the mean, providing a more robust central measure."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "df.info()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, I am not do this method for using encoding"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a column average building temperature based on all temperature\n",
        "df['Average_building_Temperature']=df[['KITCHEN_TEMP','LIVING_TEMP','BEDROOM_TEMP','OFFICE_TEMP','BATHROOM_TEMP','IRONING_ROOM_TEMP','TEEN_ROOM_2_TEMP','PARENTS_ROOM_TEMP']].mean(axis=1)\n",
        "#create a column of difference between outside and inside temperature\n",
        "df['Temperature_difference']=abs(df['Average_building_Temperature']-df['OUTSIDE_TEMP_build'])\n",
        "\n",
        "#create a column average building humidity\n",
        "df['Average_building_humidity']=df[['KITCHEN_HUM','LIVING_HUM','BEDROOM_HUM', 'OFFICE_HUM','BATHROOM_HUM','IRONING_ROOM_HUM','TEEN_ROOM_HUM','PARENTS_ROOM_HUM']].mean(axis=1)\n",
        "#create a column of difference between outside and inside building humidity\n",
        "df['Humidity_difference']=abs(df['OUTSIDE_HUM_build']-df['Average_building_humidity'])"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop random variables as it does not look like that much important while predicting the output\n",
        "columns_to_drop = ['rv1','rv2']\n",
        "df.drop(columns_to_drop, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "9LgVaX71ck14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "ViEC8Hg8ccsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate skewness for each column\n",
        "skewness = df.skew()\n",
        "\n",
        "# Visualize skewness using a histogram\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(skewness, bins=20, kde=True, color='skyblue')\n",
        "plt.title('Skewness in the Dataset')\n",
        "plt.xlabel('Skewness')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Display skewness values for each column\n",
        "print(\"Skewness for each column:\")\n",
        "print(skewness)"
      ],
      "metadata": {
        "id": "ecp7RYU3c2XN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting up the threshold\n",
        "skewness_threshold = 0.5\n",
        "\n",
        "# Separate features into symmetrical and skewed based on skewness threshold\n",
        "symmetrical_features = skewness[abs(skewness) < skewness_threshold].index\n",
        "skewed_features = skewness[abs(skewness) >= skewness_threshold].index\n",
        "\n",
        "# Create new DataFrames for symmetrical and skewed features\n",
        "print('FEATURES FOLLOWED SYMMETRICAL DISTRIBUTION :')\n",
        "symmetrical_data = df[symmetrical_features]\n",
        "print(symmetrical_features)\n",
        "\n",
        "print('FEATURES FOLLOWED SKEWED DISTRIBUTION :')\n",
        "skewed_data = df[skewed_features]\n",
        "print(skewed_features)"
      ],
      "metadata": {
        "id": "wp3jFBmuh1Rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skewed_data"
      ],
      "metadata": {
        "id": "vdrKCrp5iGKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "\n",
        "If skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\n",
        "If skewness is less than -1 or greater than 1, the distribution is highly skewed.\n",
        "\n",
        "If you find that some features are highly skewed, you might need to apply transformations (e.g., log transformation) to make the distribution more symmetrical and improve the performance of certain machine learning models."
      ],
      "metadata": {
        "id": "oUyBzxjMdD8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Required Library\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "# Initialize the PowerTransformer\n",
        "power_transform = PowerTransformer()\n",
        "\n",
        "# fit and tranform the data using the powerTransformer\n",
        "power_transformed = pd.DataFrame(power_transform.fit_transform(skewed_data))\n",
        "power_transformed.columns = skewed_data.columns\n",
        "\n",
        "# explaining the power transformed data\n",
        "power_transformed"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symmetrical_data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "Kjg6leJ0jcZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symmetrical_data"
      ],
      "metadata": {
        "id": "OpKzqv2AjTL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tranformed_data = pd.concat([symmetrical_data, power_transformed], axis=1)"
      ],
      "metadata": {
        "id": "6bXCOpCQwoEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tranformed_data.head()"
      ],
      "metadata": {
        "id": "dTM0UdDDwpda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes My data needs transformation specially skewed data , i used power transformaiton to solve this concern"
      ],
      "metadata": {
        "id": "031FczGNj9Wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# importing the desired library\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = pd.DataFrame(scaler.fit_transform(tranformed_data))\n",
        "scaled_data.columns = tranformed_data.columns\n",
        "scaled_data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is a crucial technique in machine learning and data analysis, and it is often needed for several reasons:\n",
        "\n",
        "* Curse of Dimensionality: As the number of features (dimensions) in your dataset increases, the volume of the data space grows exponentially. This can lead to sparsity, making it difficult to collect sufficient data to model relationships effectively. Dimensionality reduction can help mitigate this problem by reducing the number of features while retaining essential information.\n",
        "\n",
        "* Overfitting: Models trained on high-dimensional data are more likely to overfit the training data, meaning they perform well on training data but poorly on unseen data. By reducing dimensionality, you can reduce the complexity of the model and enhance its generalization capabilities.\n",
        "\n",
        "* Computational Efficiency: High-dimensional data can strain computational resources and increase the time required for training and prediction. Dimensionality reduction can make algorithms more efficient.\n",
        "\n",
        "* Visualization: It's challenging to visualize and interpret data in high-dimensional spaces. Reducing dimensionality allows for more accessible data exploration and visualization.\n",
        "\n",
        "* Feature Engineering: Some features may be redundant or irrelevant, and dimensionality reduction helps in identifying and removing them. This can improve model performance and understanding of data.\n",
        "\n",
        "* Collinearity: High-dimensional data often contains multicollinearity, where features are highly correlated. Dimensionality reduction can alleviate this issue and help in extracting meaningful and uncorrelated features.\n",
        "\n",
        "Common techniques for dimensionality reduction include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and feature selection methods. The choice of technique depends on the specific dataset and the goals of the analysis. In summary, dimensionality reduction is needed to simplify high-dimensional data, improve model performance, and enhance the interpretability and efficiency of data analysis."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "# Import Important Library for using the decomposition\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA instance without specifying the number of components\n",
        "pca = PCA()\n",
        "\n",
        "# fit the PCA model to your standardized data\n",
        "pca.fit(scaled_data)\n",
        "\n",
        "# Calculate the cumulative expand variance\n",
        "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Create an elbow plot to visualize the explained variance\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(range(1,len(cumulative_explained_variance) + 1), cumulative_explained_variance,marker=\"o\",linestyle=\"--\")\n",
        "plt.xlabel(\"Number of Principal Components\")\n",
        "plt.ylabel(\"Cummulative Explained Variance\")\n",
        "plt.title(\"PCA Elbow Plot\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Create a PCA instance and specify the number of components you want to retain\n",
        "# For example, if you want to retain  10 components, set n_components = 10\n",
        "n_components = 10\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# Fit the PCA model to your standardized data and transform it\n",
        "transformed_data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Explained by each components\n",
        "explained_variance = pca.explained_variance_ratio_"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The variance of the pca that we extract and there importance in predicting the output\n",
        "explained_variance"
      ],
      "metadata": {
        "id": "8bQPAsi80Vk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculating the total of explained_variance which needs to be more than 900%\n",
        "explained_variance.sum()"
      ],
      "metadata": {
        "id": "vVYUssuT0tLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# initialize a PCA instance without specifying the number components\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA model to your standardized data\n",
        "pca.fit(scaled_data)\n",
        "\n",
        "# Calculated the explained variance for each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Create a plot\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(range(1,len(explained_variance) + 1), explained_variance,marker=\"o\",linestyle=\"--\")\n",
        "plt.xlabel(\"Principal Component\")\n",
        "plt.ylabel(\"Explianed Variance\")\n",
        "plt.title(\"Scree Plot for PCA\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4l_n604Y0_yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_data_pca.shape"
      ],
      "metadata": {
        "id": "4pxK3t0K3Gdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(transformed_data_pca).head()"
      ],
      "metadata": {
        "id": "uQvkJ2zO3a2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this scenario, Principal Component Analysis (PCA) has been utilized as the dimensionality reduction technique. PCA is a widely used method for reducing the number of features in a dataset while preserving its variability and capturing the most important patterns. Here's why PCA might be chosen:\n",
        "\n",
        "Reduction of Dimensionality:\n",
        "\n",
        "PCA transforms the original features into a new set of uncorrelated variables called principal components.\n",
        "The number of principal components can be chosen to represent a significant portion of the original variability with fewer dimensions.\n",
        "\n",
        "Elimination of Redundancy:\n",
        "\n",
        "PCA identifies and removes redundancy among features by maximizing the variance captured in the new components.\n",
        "It helps in focusing on the features that contribute the most to the dataset's variability."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "# Crete a the independent and dependent variable\n",
        "X = transformed_data_pca\n",
        "y = df[\"Appliances\"]"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into 80/20 ratio\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=42)"
      ],
      "metadata": {
        "id": "JQtWach24wtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! If you have used a 0.20 splitting ratio for the train-test split (80% training, 20% testing) because 'Appliances' is the dependent variable, you can explain your choice as follows:\n",
        "\n",
        "1. **Purpose of Train-Test Split:**\n",
        "   - The train-test split is a common practice in machine learning to evaluate the model's performance on unseen data.\n",
        "   - The training set is used to train the model, and the testing set is used to assess how well the model generalizes to new, unseen data.\n",
        "\n",
        "2. **Dependent Variable ('Appliances'):**\n",
        "   - 'Appliances' is the dependent variable or the target variable that the model aims to predict.\n",
        "   - It's crucial to ensure that the model is trained on a sufficiently large portion of the data to capture patterns and relationships within the dependent variable.\n",
        "\n",
        "3. **Reasons for 80-20 Splitting Ratio:**\n",
        "   - **Data Adequacy:** An 80-20 split is a commonly used ratio, providing a substantial portion of data for training while reserving enough for testing.\n",
        "   - **Balance:** Striking a balance between having enough data to train a robust model and having a sizable test set for reliable evaluation.\n",
        "   - **Risk of Overfitting:** A smaller test set may lead to overfitting if the model memorizes the training data. A larger test set helps assess generalization performance better.\n",
        "\n",
        "4. **Model Evaluation:**\n",
        "   - The model's performance on the test set is indicative of how well it will perform on new, unseen data.\n",
        "   - The choice of a 0.20 splitting ratio allows for a reasonably large test set, enhancing the reliability of performance metrics.\n",
        "\n",
        "5. **Statistical Significance:**\n",
        "   - With a substantial portion reserved for testing, statistical significance in evaluating model performance can be achieved.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - Simple Linear Regression model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries which is preferally used\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definning the object\n",
        "reg = LinearRegression()\n",
        "reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "QLaaGL9P7HbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training dataset score\n",
        "training_score = reg.score(X_train, y_train)\n",
        "# Predicting the value\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# Calculating the training accuracy\n",
        "print(\"Train Score :\",training_score)\n",
        "\n",
        "# Calculating the testing accuracy\n",
        "r2 = r2_score(y_test,y_pred)\n",
        "print(\"R2_score :\", r2)\n",
        "\n",
        "# Calculating the Mean Squared error\n",
        "MSE = mean_squared_error(y_test,y_pred)\n",
        "print(\"The Mean Square Error is :\",MSE)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score((y_test), (y_pred)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "ioqSkAMv7Sg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Training Score (R-squared):\n",
        "- **What it Tells Us:** R-squared measures how well our model fits the training data. A high R-squared means our model does a good job explaining the patterns in the data it was trained on.\n",
        "- **Why it Matters for Business:** A high R-squared helps us understand relationships in our data. But if it's too high, it might mean our model is memorizing the training data too much, which could be a problem when we introduce new data.\n",
        "\n",
        "### Test Mean Squared Error (MSE):\n",
        "- **What it Tells Us:** MSE calculates how close our model's predictions are to the actual values in the new (test) data. Lower MSE is better.\n",
        "- **Why it Matters for Business:** A lower MSE means our model's predictions are generally close to the real values. This is good for business because it suggests our model is making reliable predictions, which can save costs and help with decision-making.\n",
        "\n",
        "### Test R-squared (R2):\n",
        "- **What it Tells Us:** R2 on the test data shows how well our model predicts new, unseen data. A higher R2 means our model is good at making predictions.\n",
        "- **Why it Matters for Business:** A high test R2 is great news. It means our model is not just good on the training data but also on new data. This is crucial for accurate forecasting and better planning, which can lead to improved business outcomes.\n",
        "\n",
        "### Business Impact Summary:\n",
        "- **Positive Aspects:** The model understands the training data well, and it predicts new data accurately. This can lead to better decision-making, efficient resource allocation, and potentially cost savings.\n",
        "- **Consideration:** We need to keep an eye on the model's performance over time. If the business environment changes, we might need to update or refine our model to ensure it continues to deliver good results.\n",
        "\n"
      ],
      "metadata": {
        "id": "4pSkrfIJ9k8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the ecvalation metric score chart\n",
        "sns.displot(y_pred - y_test, kind=\"kde\")"
      ],
      "metadata": {
        "id": "vZHgXmJg81Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot((y_pred))\n",
        "plt.plot(np.array((y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import  train_test_split, RandomizedSearchCV\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define hyperparameter search space (you can customise this based on your model)\n",
        "param_dist = {\"fit_intercept\": [True,False],\n",
        "              \"copy_X\":[True, False],\n",
        "              \"positive\":[True,False]}\n",
        "# Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
        "# Fit the Algorithm\n",
        "random_search.fit(X_train,y_train)\n",
        "\n",
        "# Get the best hyperparameter and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Train the best model with the entire training dataset\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "training_score_val = best_model.score(X_train, y_train)\n",
        "# Evaluate the best model on the test set\n",
        "test_predictions = best_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics for the test predictions (e.g., mean squared error)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "r2 = r2_score((y_test),(test_predictions))\n",
        "\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "\n",
        "#visual of training score\n",
        "print(\"Train score:\" ,training_score_val)\n",
        "print(\"Test MSE:\", mse)\n",
        "print(\"Test R2:\", r2)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(test_predictions - y_test, kind=\"kde\")"
      ],
      "metadata": {
        "id": "QnLYSDvtEj9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation and hyperparameter tunning\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(test_predictions)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dJCtD3AxE-R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon performing hyperparameter optimization using RandomizedSearchCV, there doesn't seem to be a significant improvement in model performance compared to the initial model. The training score remains the same, and the evaluation metrics on the test data also show similar values. The Mean Squared Error (MSE) and R-squared (R2) values remain approximately unchanged. This suggests that the initial model's hyperparameters were already reasonably effective, and the hyperparameter search did not lead to noticeable enhancements in this particular case. Further exploration or considering different models may be necessary to achieve substantial improvements."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - Polynomial Regression Model"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The machine learning model used here is a Polynomial Regression model with a degree of 2. This type of model is a form of linear regression that can capture more complex relationships between input features and the target variable by introducing polynomial features.\n",
        "\n",
        "### Performance Evaluation:\n",
        "\n",
        "1. **Training R-squared (R2) Score:**\n",
        "   - **Explanation:** R2 measures how well the model fits the training data. A score of 0.68 indicates that the model explains about 68% of the variance in the training data.\n",
        "   - **Meaning:** The model has a moderate fit to the training data, capturing a significant portion of the patterns present.\n",
        "\n",
        "2. **Mean Squared Error (MSE):**\n",
        "   - **Explanation:** MSE quantifies the average squared difference between predicted and actual values. A lower MSE (approximately 568.88) suggests that the model's predictions are relatively close to the actual values on average.\n",
        "   - **Meaning:** The model's predictions are accurate, with smaller errors on average.\n",
        "\n",
        "3. **Test R-squared (R2) Score:**\n",
        "   - **Explanation:** R2 on the test data measures how well the model generalizes to new, unseen data. A test R2 score of approximately 0.68, consistent with the training score, suggests good generalization.\n",
        "   - **Meaning:** The model maintains its performance on new data, indicating reliable predictions beyond the training set.\n",
        "\n"
      ],
      "metadata": {
        "id": "t7QJNyAyHIDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Required Libraries\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# as  no need to use repeate the train test split test\n",
        "# Choose the degree of polynomial\n",
        "degree = 2\n",
        "# Create a polynomial Regression model using a pipelinr\n",
        "polyreg  = make_pipeline(PolynomialFeatures(degree))"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VxxIVS6Vv4cy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}